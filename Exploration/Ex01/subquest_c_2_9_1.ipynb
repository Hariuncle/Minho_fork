{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2e4310a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c173e28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cbde8ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = diabetes.data\n",
    "\n",
    "\n",
    "df_y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a0102792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X = np.array(df_X)\n",
    "df_y = np.array(df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ff866eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, df_y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d24406c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.rand(df_X.shape[1])\n",
    "b = np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "83fa6d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X,W,b):\n",
    "    predictions = 0\n",
    "    # 열벡터인 X와 행벡터인 W간의 행렬 연산과 동일\n",
    "    for i in range(X.shape[1]):\n",
    "        predictions += X[:, i] * W[i]\n",
    "    # bias는 한번 더하기\n",
    "    predictions += b\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b32daa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(a,b):\n",
    "    # 차이의 제곱을 평균낸 것\n",
    "    mse = ((a-b) **2).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9a1c1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(X,w,b,y):\n",
    "    predictions = model(X, W, b)\n",
    "    L = MSE(predictions,y)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "46c1b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, W, b, y):\n",
    "    N = len(y)\n",
    "    \n",
    "    y_pred = model(X,W,b)\n",
    "    \n",
    "    dW = 1/N * 2 * X.T.dot(y_pred - y)\n",
    "    \n",
    "    db = 2 * (y_pred - y).mean()\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "27cf40be",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "493bb892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10 : Loss 6260.4988\n",
      "Iteration 20 : Loss 5915.6673\n",
      "Iteration 30 : Loss 5838.8350\n",
      "Iteration 40 : Loss 5767.3513\n",
      "Iteration 50 : Loss 5698.1116\n",
      "Iteration 60 : Loss 5631.0060\n",
      "Iteration 70 : Loss 5565.9619\n",
      "Iteration 80 : Loss 5502.9095\n",
      "Iteration 90 : Loss 5441.7815\n",
      "Iteration 100 : Loss 5382.5128\n",
      "Iteration 110 : Loss 5325.0408\n",
      "Iteration 120 : Loss 5269.3050\n",
      "Iteration 130 : Loss 5215.2469\n",
      "Iteration 140 : Loss 5162.8101\n",
      "Iteration 150 : Loss 5111.9403\n",
      "Iteration 160 : Loss 5062.5849\n",
      "Iteration 170 : Loss 5014.6932\n",
      "Iteration 180 : Loss 4968.2164\n",
      "Iteration 190 : Loss 4923.1071\n",
      "Iteration 200 : Loss 4879.3198\n",
      "Iteration 210 : Loss 4836.8105\n",
      "Iteration 220 : Loss 4795.5368\n",
      "Iteration 230 : Loss 4755.4576\n",
      "Iteration 240 : Loss 4716.5334\n",
      "Iteration 250 : Loss 4678.7260\n",
      "Iteration 260 : Loss 4641.9984\n",
      "Iteration 270 : Loss 4606.3152\n",
      "Iteration 280 : Loss 4571.6419\n",
      "Iteration 290 : Loss 4537.9453\n",
      "Iteration 300 : Loss 4505.1935\n",
      "Iteration 310 : Loss 4473.3555\n",
      "Iteration 320 : Loss 4442.4015\n",
      "Iteration 330 : Loss 4412.3026\n",
      "Iteration 340 : Loss 4383.0311\n",
      "Iteration 350 : Loss 4354.5599\n",
      "Iteration 360 : Loss 4326.8633\n",
      "Iteration 370 : Loss 4299.9162\n",
      "Iteration 380 : Loss 4273.6943\n",
      "Iteration 390 : Loss 4248.1743\n",
      "Iteration 400 : Loss 4223.3338\n",
      "Iteration 410 : Loss 4199.1508\n",
      "Iteration 420 : Loss 4175.6043\n",
      "Iteration 430 : Loss 4152.6741\n",
      "Iteration 440 : Loss 4130.3406\n",
      "Iteration 450 : Loss 4108.5848\n",
      "Iteration 460 : Loss 4087.3883\n",
      "Iteration 470 : Loss 4066.7336\n",
      "Iteration 480 : Loss 4046.6036\n",
      "Iteration 490 : Loss 4026.9817\n",
      "Iteration 500 : Loss 4007.8521\n",
      "Iteration 510 : Loss 3989.1994\n",
      "Iteration 520 : Loss 3971.0087\n",
      "Iteration 530 : Loss 3953.2657\n",
      "Iteration 540 : Loss 3935.9565\n",
      "Iteration 550 : Loss 3919.0677\n",
      "Iteration 560 : Loss 3902.5864\n",
      "Iteration 570 : Loss 3886.5000\n",
      "Iteration 580 : Loss 3870.7966\n",
      "Iteration 590 : Loss 3855.4643\n",
      "Iteration 600 : Loss 3840.4920\n",
      "Iteration 610 : Loss 3825.8687\n",
      "Iteration 620 : Loss 3811.5840\n",
      "Iteration 630 : Loss 3797.6275\n",
      "Iteration 640 : Loss 3783.9896\n",
      "Iteration 650 : Loss 3770.6606\n",
      "Iteration 660 : Loss 3757.6314\n",
      "Iteration 670 : Loss 3744.8932\n",
      "Iteration 680 : Loss 3732.4372\n",
      "Iteration 690 : Loss 3720.2553\n",
      "Iteration 700 : Loss 3708.3394\n",
      "Iteration 710 : Loss 3696.6817\n",
      "Iteration 720 : Loss 3685.2747\n",
      "Iteration 730 : Loss 3674.1113\n",
      "Iteration 740 : Loss 3663.1843\n",
      "Iteration 750 : Loss 3652.4870\n",
      "Iteration 760 : Loss 3642.0129\n",
      "Iteration 770 : Loss 3631.7556\n",
      "Iteration 780 : Loss 3621.7089\n",
      "Iteration 790 : Loss 3611.8670\n",
      "Iteration 800 : Loss 3602.2240\n",
      "Iteration 810 : Loss 3592.7746\n",
      "Iteration 820 : Loss 3583.5132\n",
      "Iteration 830 : Loss 3574.4346\n",
      "Iteration 840 : Loss 3565.5340\n",
      "Iteration 850 : Loss 3556.8063\n",
      "Iteration 860 : Loss 3548.2469\n",
      "Iteration 870 : Loss 3539.8513\n",
      "Iteration 880 : Loss 3531.6150\n",
      "Iteration 890 : Loss 3523.5337\n",
      "Iteration 900 : Loss 3515.6034\n",
      "Iteration 910 : Loss 3507.8201\n",
      "Iteration 920 : Loss 3500.1797\n",
      "Iteration 930 : Loss 3492.6788\n",
      "Iteration 940 : Loss 3485.3135\n",
      "Iteration 950 : Loss 3478.0804\n",
      "Iteration 960 : Loss 3470.9760\n",
      "Iteration 970 : Loss 3463.9972\n",
      "Iteration 980 : Loss 3457.1407\n",
      "Iteration 990 : Loss 3450.4033\n",
      "Iteration 1000 : Loss 3443.7822\n",
      "Iteration 1010 : Loss 3437.2744\n",
      "Iteration 1020 : Loss 3430.8771\n",
      "Iteration 1030 : Loss 3424.5876\n",
      "Iteration 1040 : Loss 3418.4032\n",
      "Iteration 1050 : Loss 3412.3213\n",
      "Iteration 1060 : Loss 3406.3396\n",
      "Iteration 1070 : Loss 3400.4556\n",
      "Iteration 1080 : Loss 3394.6669\n",
      "Iteration 1090 : Loss 3388.9714\n",
      "Iteration 1100 : Loss 3383.3668\n",
      "Iteration 1110 : Loss 3377.8509\n",
      "Iteration 1120 : Loss 3372.4218\n",
      "Iteration 1130 : Loss 3367.0774\n",
      "Iteration 1140 : Loss 3361.8158\n",
      "Iteration 1150 : Loss 3356.6351\n",
      "Iteration 1160 : Loss 3351.5335\n",
      "Iteration 1170 : Loss 3346.5092\n",
      "Iteration 1180 : Loss 3341.5605\n",
      "Iteration 1190 : Loss 3336.6858\n",
      "Iteration 1200 : Loss 3331.8833\n",
      "Iteration 1210 : Loss 3327.1515\n",
      "Iteration 1220 : Loss 3322.4890\n",
      "Iteration 1230 : Loss 3317.8942\n",
      "Iteration 1240 : Loss 3313.3656\n",
      "Iteration 1250 : Loss 3308.9019\n",
      "Iteration 1260 : Loss 3304.5018\n",
      "Iteration 1270 : Loss 3300.1638\n",
      "Iteration 1280 : Loss 3295.8867\n",
      "Iteration 1290 : Loss 3291.6693\n",
      "Iteration 1300 : Loss 3287.5104\n",
      "Iteration 1310 : Loss 3283.4087\n",
      "Iteration 1320 : Loss 3279.3631\n",
      "Iteration 1330 : Loss 3275.3725\n",
      "Iteration 1340 : Loss 3271.4359\n",
      "Iteration 1350 : Loss 3267.5521\n",
      "Iteration 1360 : Loss 3263.7202\n",
      "Iteration 1370 : Loss 3259.9391\n",
      "Iteration 1380 : Loss 3256.2078\n",
      "Iteration 1390 : Loss 3252.5255\n",
      "Iteration 1400 : Loss 3248.8912\n",
      "Iteration 1410 : Loss 3245.3040\n",
      "Iteration 1420 : Loss 3241.7630\n",
      "Iteration 1430 : Loss 3238.2674\n",
      "Iteration 1440 : Loss 3234.8163\n",
      "Iteration 1450 : Loss 3231.4090\n",
      "Iteration 1460 : Loss 3228.0447\n",
      "Iteration 1470 : Loss 3224.7225\n",
      "Iteration 1480 : Loss 3221.4418\n",
      "Iteration 1490 : Loss 3218.2018\n",
      "Iteration 1500 : Loss 3215.0018\n",
      "Iteration 1510 : Loss 3211.8412\n",
      "Iteration 1520 : Loss 3208.7191\n",
      "Iteration 1530 : Loss 3205.6351\n",
      "Iteration 1540 : Loss 3202.5884\n",
      "Iteration 1550 : Loss 3199.5784\n",
      "Iteration 1560 : Loss 3196.6045\n",
      "Iteration 1570 : Loss 3193.6661\n",
      "Iteration 1580 : Loss 3190.7626\n",
      "Iteration 1590 : Loss 3187.8935\n",
      "Iteration 1600 : Loss 3185.0582\n",
      "Iteration 1610 : Loss 3182.2562\n",
      "Iteration 1620 : Loss 3179.4868\n",
      "Iteration 1630 : Loss 3176.7497\n",
      "Iteration 1640 : Loss 3174.0443\n",
      "Iteration 1650 : Loss 3171.3701\n",
      "Iteration 1660 : Loss 3168.7266\n",
      "Iteration 1670 : Loss 3166.1134\n",
      "Iteration 1680 : Loss 3163.5300\n",
      "Iteration 1690 : Loss 3160.9759\n",
      "Iteration 1700 : Loss 3158.4507\n",
      "Iteration 1710 : Loss 3155.9540\n",
      "Iteration 1720 : Loss 3153.4853\n",
      "Iteration 1730 : Loss 3151.0442\n",
      "Iteration 1740 : Loss 3148.6304\n",
      "Iteration 1750 : Loss 3146.2434\n",
      "Iteration 1760 : Loss 3143.8828\n",
      "Iteration 1770 : Loss 3141.5483\n",
      "Iteration 1780 : Loss 3139.2395\n",
      "Iteration 1790 : Loss 3136.9560\n",
      "Iteration 1800 : Loss 3134.6975\n",
      "Iteration 1810 : Loss 3132.4636\n",
      "Iteration 1820 : Loss 3130.2539\n",
      "Iteration 1830 : Loss 3128.0682\n",
      "Iteration 1840 : Loss 3125.9061\n",
      "Iteration 1850 : Loss 3123.7673\n",
      "Iteration 1860 : Loss 3121.6515\n",
      "Iteration 1870 : Loss 3119.5583\n",
      "Iteration 1880 : Loss 3117.4874\n",
      "Iteration 1890 : Loss 3115.4387\n",
      "Iteration 1900 : Loss 3113.4117\n",
      "Iteration 1910 : Loss 3111.4061\n",
      "Iteration 1920 : Loss 3109.4218\n",
      "Iteration 1930 : Loss 3107.4583\n",
      "Iteration 1940 : Loss 3105.5155\n",
      "Iteration 1950 : Loss 3103.5931\n",
      "Iteration 1960 : Loss 3101.6908\n",
      "Iteration 1970 : Loss 3099.8084\n",
      "Iteration 1980 : Loss 3097.9455\n",
      "Iteration 1990 : Loss 3096.1020\n",
      "Iteration 2000 : Loss 3094.2776\n",
      "Iteration 2010 : Loss 3092.4721\n",
      "Iteration 2020 : Loss 3090.6852\n",
      "Iteration 2030 : Loss 3088.9167\n",
      "Iteration 2040 : Loss 3087.1664\n",
      "Iteration 2050 : Loss 3085.4341\n",
      "Iteration 2060 : Loss 3083.7194\n",
      "Iteration 2070 : Loss 3082.0223\n",
      "Iteration 2080 : Loss 3080.3425\n",
      "Iteration 2090 : Loss 3078.6798\n",
      "Iteration 2100 : Loss 3077.0340\n",
      "Iteration 2110 : Loss 3075.4048\n",
      "Iteration 2120 : Loss 3073.7921\n",
      "Iteration 2130 : Loss 3072.1957\n",
      "Iteration 2140 : Loss 3070.6155\n",
      "Iteration 2150 : Loss 3069.0511\n",
      "Iteration 2160 : Loss 3067.5024\n",
      "Iteration 2170 : Loss 3065.9692\n",
      "Iteration 2180 : Loss 3064.4515\n",
      "Iteration 2190 : Loss 3062.9488\n",
      "Iteration 2200 : Loss 3061.4612\n",
      "Iteration 2210 : Loss 3059.9884\n",
      "Iteration 2220 : Loss 3058.5302\n",
      "Iteration 2230 : Loss 3057.0866\n",
      "Iteration 2240 : Loss 3055.6572\n",
      "Iteration 2250 : Loss 3054.2420\n",
      "Iteration 2260 : Loss 3052.8408\n",
      "Iteration 2270 : Loss 3051.4535\n",
      "Iteration 2280 : Loss 3050.0798\n",
      "Iteration 2290 : Loss 3048.7196\n",
      "Iteration 2300 : Loss 3047.3728\n",
      "Iteration 2310 : Loss 3046.0392\n",
      "Iteration 2320 : Loss 3044.7187\n",
      "Iteration 2330 : Loss 3043.4112\n",
      "Iteration 2340 : Loss 3042.1164\n",
      "Iteration 2350 : Loss 3040.8343\n",
      "Iteration 2360 : Loss 3039.5646\n",
      "Iteration 2370 : Loss 3038.3074\n",
      "Iteration 2380 : Loss 3037.0624\n",
      "Iteration 2390 : Loss 3035.8294\n",
      "Iteration 2400 : Loss 3034.6085\n",
      "Iteration 2410 : Loss 3033.3994\n",
      "Iteration 2420 : Loss 3032.2019\n",
      "Iteration 2430 : Loss 3031.0161\n",
      "Iteration 2440 : Loss 3029.8417\n",
      "Iteration 2450 : Loss 3028.6787\n",
      "Iteration 2460 : Loss 3027.5268\n",
      "Iteration 2470 : Loss 3026.3861\n",
      "Iteration 2480 : Loss 3025.2563\n",
      "Iteration 2490 : Loss 3024.1374\n",
      "Iteration 2500 : Loss 3023.0292\n",
      "Iteration 2510 : Loss 3021.9317\n",
      "Iteration 2520 : Loss 3020.8446\n",
      "Iteration 2530 : Loss 3019.7680\n",
      "Iteration 2540 : Loss 3018.7016\n",
      "Iteration 2550 : Loss 3017.6454\n",
      "Iteration 2560 : Loss 3016.5993\n",
      "Iteration 2570 : Loss 3015.5632\n",
      "Iteration 2580 : Loss 3014.5369\n",
      "Iteration 2590 : Loss 3013.5204\n",
      "Iteration 2600 : Loss 3012.5135\n",
      "Iteration 2610 : Loss 3011.5162\n",
      "Iteration 2620 : Loss 3010.5284\n",
      "Iteration 2630 : Loss 3009.5499\n",
      "Iteration 2640 : Loss 3008.5807\n",
      "Iteration 2650 : Loss 3007.6206\n",
      "Iteration 2660 : Loss 3006.6696\n",
      "Iteration 2670 : Loss 3005.7276\n",
      "Iteration 2680 : Loss 3004.7945\n",
      "Iteration 2690 : Loss 3003.8701\n",
      "Iteration 2700 : Loss 3002.9545\n",
      "Iteration 2710 : Loss 3002.0475\n",
      "Iteration 2720 : Loss 3001.1490\n",
      "Iteration 2730 : Loss 3000.2589\n",
      "Iteration 2740 : Loss 2999.3772\n",
      "Iteration 2750 : Loss 2998.5038\n",
      "Iteration 2760 : Loss 2997.6385\n",
      "Iteration 2770 : Loss 2996.7813\n",
      "Iteration 2780 : Loss 2995.9322\n",
      "Iteration 2790 : Loss 2995.0910\n",
      "Iteration 2800 : Loss 2994.2576\n",
      "Iteration 2810 : Loss 2993.4320\n",
      "Iteration 2820 : Loss 2992.6141\n",
      "Iteration 2830 : Loss 2991.8038\n",
      "Iteration 2840 : Loss 2991.0011\n",
      "Iteration 2850 : Loss 2990.2058\n",
      "Iteration 2860 : Loss 2989.4180\n",
      "Iteration 2870 : Loss 2988.6374\n",
      "Iteration 2880 : Loss 2987.8641\n",
      "Iteration 2890 : Loss 2987.0979\n",
      "Iteration 2900 : Loss 2986.3388\n",
      "Iteration 2910 : Loss 2985.5868\n",
      "Iteration 2920 : Loss 2984.8417\n",
      "Iteration 2930 : Loss 2984.1035\n",
      "Iteration 2940 : Loss 2983.3722\n",
      "Iteration 2950 : Loss 2982.6475\n",
      "Iteration 2960 : Loss 2981.9296\n",
      "Iteration 2970 : Loss 2981.2183\n",
      "Iteration 2980 : Loss 2980.5135\n",
      "Iteration 2990 : Loss 2979.8152\n",
      "Iteration 3000 : Loss 2979.1233\n",
      "Iteration 3010 : Loss 2978.4378\n",
      "Iteration 3020 : Loss 2977.7586\n",
      "Iteration 3030 : Loss 2977.0856\n",
      "Iteration 3040 : Loss 2976.4187\n",
      "Iteration 3050 : Loss 2975.7580\n",
      "Iteration 3060 : Loss 2975.1034\n",
      "Iteration 3070 : Loss 2974.4547\n",
      "Iteration 3080 : Loss 2973.8119\n",
      "Iteration 3090 : Loss 2973.1751\n",
      "Iteration 3100 : Loss 2972.5440\n",
      "Iteration 3110 : Loss 2971.9187\n",
      "Iteration 3120 : Loss 2971.2991\n",
      "Iteration 3130 : Loss 2970.6851\n",
      "Iteration 3140 : Loss 2970.0768\n",
      "Iteration 3150 : Loss 2969.4739\n",
      "Iteration 3160 : Loss 2968.8766\n",
      "Iteration 3170 : Loss 2968.2847\n",
      "Iteration 3180 : Loss 2967.6981\n",
      "Iteration 3190 : Loss 2967.1169\n",
      "Iteration 3200 : Loss 2966.5409\n",
      "Iteration 3210 : Loss 2965.9702\n",
      "Iteration 3220 : Loss 2965.4046\n",
      "Iteration 3230 : Loss 2964.8441\n",
      "Iteration 3240 : Loss 2964.2887\n",
      "Iteration 3250 : Loss 2963.7384\n",
      "Iteration 3260 : Loss 2963.1930\n",
      "Iteration 3270 : Loss 2962.6525\n",
      "Iteration 3280 : Loss 2962.1169\n",
      "Iteration 3290 : Loss 2961.5861\n",
      "Iteration 3300 : Loss 2961.0601\n",
      "Iteration 3310 : Loss 2960.5388\n",
      "Iteration 3320 : Loss 2960.0222\n",
      "Iteration 3330 : Loss 2959.5103\n",
      "Iteration 3340 : Loss 2959.0029\n",
      "Iteration 3350 : Loss 2958.5001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3360 : Loss 2958.0018\n",
      "Iteration 3370 : Loss 2957.5080\n",
      "Iteration 3380 : Loss 2957.0186\n",
      "Iteration 3390 : Loss 2956.5336\n",
      "Iteration 3400 : Loss 2956.0529\n",
      "Iteration 3410 : Loss 2955.5766\n",
      "Iteration 3420 : Loss 2955.1044\n",
      "Iteration 3430 : Loss 2954.6365\n",
      "Iteration 3440 : Loss 2954.1728\n",
      "Iteration 3450 : Loss 2953.7132\n",
      "Iteration 3460 : Loss 2953.2577\n",
      "Iteration 3470 : Loss 2952.8062\n",
      "Iteration 3480 : Loss 2952.3588\n",
      "Iteration 3490 : Loss 2951.9154\n",
      "Iteration 3500 : Loss 2951.4758\n",
      "Iteration 3510 : Loss 2951.0402\n",
      "Iteration 3520 : Loss 2950.6085\n",
      "Iteration 3530 : Loss 2950.1806\n",
      "Iteration 3540 : Loss 2949.7564\n",
      "Iteration 3550 : Loss 2949.3361\n",
      "Iteration 3560 : Loss 2948.9194\n",
      "Iteration 3570 : Loss 2948.5064\n",
      "Iteration 3580 : Loss 2948.0971\n",
      "Iteration 3590 : Loss 2947.6914\n",
      "Iteration 3600 : Loss 2947.2893\n",
      "Iteration 3610 : Loss 2946.8907\n",
      "Iteration 3620 : Loss 2946.4957\n",
      "Iteration 3630 : Loss 2946.1041\n",
      "Iteration 3640 : Loss 2945.7159\n",
      "Iteration 3650 : Loss 2945.3312\n",
      "Iteration 3660 : Loss 2944.9499\n",
      "Iteration 3670 : Loss 2944.5719\n",
      "Iteration 3680 : Loss 2944.1972\n",
      "Iteration 3690 : Loss 2943.8258\n",
      "Iteration 3700 : Loss 2943.4577\n",
      "Iteration 3710 : Loss 2943.0928\n",
      "Iteration 3720 : Loss 2942.7311\n",
      "Iteration 3730 : Loss 2942.3726\n",
      "Iteration 3740 : Loss 2942.0171\n",
      "Iteration 3750 : Loss 2941.6648\n",
      "Iteration 3760 : Loss 2941.3156\n",
      "Iteration 3770 : Loss 2940.9694\n",
      "Iteration 3780 : Loss 2940.6263\n",
      "Iteration 3790 : Loss 2940.2861\n",
      "Iteration 3800 : Loss 2939.9489\n",
      "Iteration 3810 : Loss 2939.6146\n",
      "Iteration 3820 : Loss 2939.2833\n",
      "Iteration 3830 : Loss 2938.9548\n",
      "Iteration 3840 : Loss 2938.6292\n",
      "Iteration 3850 : Loss 2938.3063\n",
      "Iteration 3860 : Loss 2937.9863\n",
      "Iteration 3870 : Loss 2937.6691\n",
      "Iteration 3880 : Loss 2937.3546\n",
      "Iteration 3890 : Loss 2937.0429\n",
      "Iteration 3900 : Loss 2936.7338\n",
      "Iteration 3910 : Loss 2936.4274\n",
      "Iteration 3920 : Loss 2936.1236\n",
      "Iteration 3930 : Loss 2935.8225\n",
      "Iteration 3940 : Loss 2935.5240\n",
      "Iteration 3950 : Loss 2935.2280\n",
      "Iteration 3960 : Loss 2934.9346\n",
      "Iteration 3970 : Loss 2934.6438\n",
      "Iteration 3980 : Loss 2934.3554\n",
      "Iteration 3990 : Loss 2934.0695\n",
      "Iteration 4000 : Loss 2933.7860\n",
      "Iteration 4010 : Loss 2933.5050\n",
      "Iteration 4020 : Loss 2933.2264\n",
      "Iteration 4030 : Loss 2932.9502\n",
      "Iteration 4040 : Loss 2932.6764\n",
      "Iteration 4050 : Loss 2932.4049\n",
      "Iteration 4060 : Loss 2932.1357\n",
      "Iteration 4070 : Loss 2931.8688\n",
      "Iteration 4080 : Loss 2931.6042\n",
      "Iteration 4090 : Loss 2931.3419\n",
      "Iteration 4100 : Loss 2931.0818\n",
      "Iteration 4110 : Loss 2930.8239\n",
      "Iteration 4120 : Loss 2930.5682\n",
      "Iteration 4130 : Loss 2930.3147\n",
      "Iteration 4140 : Loss 2930.0633\n",
      "Iteration 4150 : Loss 2929.8141\n",
      "Iteration 4160 : Loss 2929.5670\n",
      "Iteration 4170 : Loss 2929.3220\n",
      "Iteration 4180 : Loss 2929.0791\n",
      "Iteration 4190 : Loss 2928.8382\n",
      "Iteration 4200 : Loss 2928.5994\n",
      "Iteration 4210 : Loss 2928.3626\n",
      "Iteration 4220 : Loss 2928.1278\n",
      "Iteration 4230 : Loss 2927.8950\n",
      "Iteration 4240 : Loss 2927.6641\n",
      "Iteration 4250 : Loss 2927.4353\n",
      "Iteration 4260 : Loss 2927.2083\n",
      "Iteration 4270 : Loss 2926.9832\n",
      "Iteration 4280 : Loss 2926.7601\n",
      "Iteration 4290 : Loss 2926.5388\n",
      "Iteration 4300 : Loss 2926.3194\n",
      "Iteration 4310 : Loss 2926.1018\n",
      "Iteration 4320 : Loss 2925.8860\n",
      "Iteration 4330 : Loss 2925.6721\n",
      "Iteration 4340 : Loss 2925.4600\n",
      "Iteration 4350 : Loss 2925.2496\n",
      "Iteration 4360 : Loss 2925.0410\n",
      "Iteration 4370 : Loss 2924.8341\n",
      "Iteration 4380 : Loss 2924.6290\n",
      "Iteration 4390 : Loss 2924.4256\n",
      "Iteration 4400 : Loss 2924.2239\n",
      "Iteration 4410 : Loss 2924.0238\n",
      "Iteration 4420 : Loss 2923.8255\n",
      "Iteration 4430 : Loss 2923.6288\n",
      "Iteration 4440 : Loss 2923.4337\n",
      "Iteration 4450 : Loss 2923.2402\n",
      "Iteration 4460 : Loss 2923.0484\n",
      "Iteration 4470 : Loss 2922.8581\n",
      "Iteration 4480 : Loss 2922.6695\n",
      "Iteration 4490 : Loss 2922.4823\n",
      "Iteration 4500 : Loss 2922.2968\n",
      "Iteration 4510 : Loss 2922.1128\n",
      "Iteration 4520 : Loss 2921.9303\n",
      "Iteration 4530 : Loss 2921.7493\n",
      "Iteration 4540 : Loss 2921.5698\n",
      "Iteration 4550 : Loss 2921.3918\n",
      "Iteration 4560 : Loss 2921.2152\n",
      "Iteration 4570 : Loss 2921.0402\n",
      "Iteration 4580 : Loss 2920.8665\n",
      "Iteration 4590 : Loss 2920.6943\n",
      "Iteration 4600 : Loss 2920.5235\n",
      "Iteration 4610 : Loss 2920.3541\n",
      "Iteration 4620 : Loss 2920.1861\n",
      "Iteration 4630 : Loss 2920.0195\n",
      "Iteration 4640 : Loss 2919.8542\n",
      "Iteration 4650 : Loss 2919.6903\n",
      "Iteration 4660 : Loss 2919.5278\n",
      "Iteration 4670 : Loss 2919.3665\n",
      "Iteration 4680 : Loss 2919.2066\n",
      "Iteration 4690 : Loss 2919.0480\n",
      "Iteration 4700 : Loss 2918.8907\n",
      "Iteration 4710 : Loss 2918.7347\n",
      "Iteration 4720 : Loss 2918.5799\n",
      "Iteration 4730 : Loss 2918.4264\n",
      "Iteration 4740 : Loss 2918.2742\n",
      "Iteration 4750 : Loss 2918.1232\n",
      "Iteration 4760 : Loss 2917.9734\n",
      "Iteration 4770 : Loss 2917.8248\n",
      "Iteration 4780 : Loss 2917.6775\n",
      "Iteration 4790 : Loss 2917.5313\n",
      "Iteration 4800 : Loss 2917.3863\n",
      "Iteration 4810 : Loss 2917.2425\n",
      "Iteration 4820 : Loss 2917.0999\n",
      "Iteration 4830 : Loss 2916.9584\n",
      "Iteration 4840 : Loss 2916.8181\n",
      "Iteration 4850 : Loss 2916.6788\n",
      "Iteration 4860 : Loss 2916.5407\n",
      "Iteration 4870 : Loss 2916.4038\n",
      "Iteration 4880 : Loss 2916.2679\n",
      "Iteration 4890 : Loss 2916.1331\n",
      "Iteration 4900 : Loss 2915.9994\n",
      "Iteration 4910 : Loss 2915.8668\n",
      "Iteration 4920 : Loss 2915.7352\n",
      "Iteration 4930 : Loss 2915.6047\n",
      "Iteration 4940 : Loss 2915.4752\n",
      "Iteration 4950 : Loss 2915.3468\n",
      "Iteration 4960 : Loss 2915.2194\n",
      "Iteration 4970 : Loss 2915.0930\n",
      "Iteration 4980 : Loss 2914.9676\n",
      "Iteration 4990 : Loss 2914.8433\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "# 하이퍼파라미터 조정을 위한 데이터 초기화\n",
    "W = np.random.rand(df_X.shape[1])\n",
    "b = np.random.rand()\n",
    "\n",
    "# epoch 설정\n",
    "epoch = 5000\n",
    "\n",
    "for i in range (1, epoch):\n",
    "\t# gradient 계산\n",
    "\tdW, db = gradient(X_train, W, b, y_train)\n",
    "\t\n",
    "\t# W와 b에 대해 gradient decent 수행\n",
    "\tW -= LEARNING_RATE * dW\n",
    "\tb -= LEARNING_RATE * db\n",
    "\t\n",
    "\t# loss의 기록 남기기\n",
    "\tL = Loss(X_train, W, b, y_train)\n",
    "\tlosses.append(L)\n",
    "\t\n",
    "\tif i % 10 == 0:\n",
    "\t\tprint('Iteration %d : Loss %0.4f' % (i, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "89491144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2867.0014171313946\n"
     ]
    }
   ],
   "source": [
    "prediction = model(X_test, W, b)\n",
    "mse = Loss(X_test, W, b, y_test)\n",
    "print(f\"MSE: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b0a9906f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_test[:, 0], y_test, label=\"target\", alpha=0.7, color='blue')  \n",
    "plt.scatter(X_test[:, 0], prediction, label=\"prediction\", alpha=0.7, color='red')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b3d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
